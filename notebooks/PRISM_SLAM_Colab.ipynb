{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "name": "Hybrid_Primitive_SLAM_Colab.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Hybrid Scene-Coordinate + Primitive-Shape Mapping for Compact, Navigation-Grade SLAM\n**A Colab-style short paper with dataset & model I/O specs**  \n*Draft \u2014 August 2025*\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Abstract\nWe propose a hybrid visual localization and mapping approach that combines (i) a **scene-coordinate head** for geometry-tight 6-DoF pose estimation via PnP/RANSAC, and (ii) a **primitive-shape head** that predicts a small set of parametric 3D shapes (planes, cuboids, cylinders, superquadrics). The result is a **compact global map** that is well-suited for navigation and planning, without photorealistic detail or large point clouds.\nWe provide a concrete dataset format (files, JSON schemas), precise **model inputs/outputs**, loss definitions, training schedule, and evaluation metrics. The notebook also includes reference code stubs for dataset validation, primitive parameterization, silhouette rasterization to a coarse grid, and set-matching (Hungarian fallback).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Introduction\nTraditional structure-from-motion and SLAM systems often accumulate large **point clouds** or **voxel volumes**.\nFor embedded navigation, we target a **compact map** made of **few parametric shapes** while keeping tight pose accuracy.\nWe adopt a **hybrid** design:\n(1) a **Scene-Coordinate Regression** head predicts sparse 2D\u21923D correspondences to recover pose with PnP(+RANSAC/DSAC), and\n(2) a **Set-Prediction Primitive** head outputs a small number of 3D shapes that fuse over time into a **shape-graph** map.\nDynamic objects are masked and excluded from mapping. The final map is tiny (dozens\u2013hundreds of parameters per scene) yet supports planning (free-space, clearance).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Related Work (very brief)\n- **Scene Coordinates & DSAC/DSAC++**: Learn pixel\u2192world 3D mappings enabling PnP; strong for precise pose.\n- **Primitive-based Mapping**: Plane/Cuboid/Cylinder fitting (e.g., PlaneRCNN, CubeSLAM), and shape abstraction via superquadrics/quadric assemblies for compact modeling.\n- **Set Prediction**: DETR-style bipartite matching for variable-cardinality object sets, used here for 3D geometric primitives.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Approach Overview\nAt each frame, given RGB, intrinsics **K**, and a dynamic mask (optional), the shared backbone produces features for two heads:\n1) **Scene-Coord Head (pose):** predicts 3D world coordinates for a sparse set of pixels and per-point uncertainty. Pose is estimated with PnP + (differentiable) RANSAC (DSAC++).  \n2) **Primitive Head (map):** predicts up to **M** parametric shapes with a set-prediction head. Each primitive is in the **camera frame** and then transformed to **world** via the estimated pose.\nWe data-associate predicted shapes to a persistent **shape-graph**, refine parameters, and perform loop-closure with image retrieval and pose-graph optimization (Sim(3) for monocular).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Dataset Specification\n\n### 4.1 Coordinate Conventions\n- **Camera (OpenCV):** +x right, +y down, +z forward.  \n- **World (ENU):** +X east, +Y north, +Z up.  \n- We store poses as **T_cw** (camera in world) unless stated otherwise.\n\n### 4.2 Directory Layout\n```\ndataset_root/\n  scene_000/\n    images/{frame:06d}.png\n    depth/{frame:06d}.exr            # optional, meters\n    dyn_mask/{frame:06d}.png         # optional, 1=dynamic\n    meta/{frame:06d}.json            # per-frame metadata (K, T_cw, etc.)\n    primitives/{frame:06d}.json      # per-frame primitive GT (optional if per-scene file used)\n  scene_000_primitives.json          # optional: per-scene GT primitives\n  schema/                            # JSON schemas (frames, primitives)\n```\n\n### 4.3 Per-Frame Metadata (`meta/{frame}.json`)\n```json\n{\n  \"frame_id\": 123,\n  \"timestamp\": 1725000000.033,\n  \"rgb_path\": \"images/000123.png\",\n  \"depth_path\": \"depth/000123.exr\",\n  \"dyn_mask_path\": \"dyn_mask/000123.png\",\n  \"K\": [[fx,0,cx],[0,fy,cy],[0,0,1]],\n  \"T_cw\": [[r11,r12,r13,tx],[r21,r22,r23,ty],[r31,r32,r33,tz],[0,0,0,1]],\n  \"scene_id\": \"scene_000\",\n  \"split\": \"train\",\n  \"notes\": \"\"\n}\n```\n\n### 4.4 Scene-Coordinate GT (sparse)\n- For a sparse grid (e.g., every 4th pixel) or a fixed set K per frame, store valid 2D\u21923D pairs and (optional) per-point covariance.\n- File: `scene_coords/{frame}.npz` with arrays:\n  - `uv` : shape (N,2) float32 pixel coords\n  - `XYZ`: shape (N,3) float32 world points\n  - `valid`: shape (N,) uint8\n  - `cov`: shape (N,3,3) float32 (optional)\n\n### 4.5 Primitive Ground Truth\nA primitive is a dict with at least: `id`, `type`, `params`, `frame_id` (if per-frame) or without `frame_id` if per-scene static.\n\n**Supported primitive types and parameters (world frame unless noted):**\n- `plane`: normal (nx,ny,nz), offset `d` (s.t. n\u00b7X + d = 0), in-plane extents `(sx, sy)` and orientation for extents `(u,v)` if not axis-aligned.\n- `cuboid`: center `c=(cx,cy,cz)`, size `s=(sx,sy,sz)`, rotation `q=(qw,qx,qy,qz)`.\n- `cylinder`: point on axis `p`, axis direction `v` (unit), radius `r`, height `h`.\n- `superquadric`: center `c`, rotation `q`, semi-axes `(a1,a2,a3)`, exponents `(eps1,eps2)`.\n\n**Example `primitives/{frame}.json`:**\n```json\n{\n  \"frame_id\": 123,\n  \"primitives\": [\n    {\"id\":\"pl_0\",\"type\":\"plane\",\n     \"params\":{\"n\":[0,0,1],\"d\":-0.0,\"sx\":20.0,\"sy\":20.0}},\n    {\"id\":\"cb_1\",\"type\":\"cuboid\",\n     \"params\":{\"c\":[5.0,2.0,0.0],\"s\":[4.0,3.0,10.0],\"q\":[1,0,0,0]}}\n  ]\n}\n```\n\n**Optional per-scene GT (`scene_XXX_primitives.json`)** for static structures:\n```json\n{\n  \"scene_id\":\"scene_000\",\n  \"primitives\":[ ... ]  // same structure as above, no frame_id\n}\n```\n\n### 4.6 JSON Schemas\nWe provide JSON Schemas in this notebook to validate `meta/*.json` and `primitives/*.json` during data prep.\n\n### 4.7 Dynamic Masks\nBinary mask (1=dynamic). Used to ignore pixels for scene-coordinates and to avoid fitting primitives to movers.\n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {
          "elapsed": 0
        }
      },
      "source": "# (Optional) Colab: install extras if needed\n# !pip install numpy pillow shapely==2.0.4\n# If using SciPy for Hungarian: !pip install scipy\n# If using Pydantic:           !pip install pydantic\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {
          "elapsed": 0
        }
      },
      "source": "import json, os, math, numpy as np\nfrom typing import Dict, List, Tuple, Optional\n\n# ---- JSON Schemas (lightweight) ----\n\nFRAME_META_SCHEMA = {\n    \"type\": \"object\",\n    \"required\": [\"frame_id\",\"rgb_path\",\"K\",\"T_cw\",\"scene_id\"],\n    \"properties\": {\n        \"frame_id\": {\"type\":\"integer\"},\n        \"timestamp\": {\"type\":[\"number\",\"integer\"]},\n        \"rgb_path\": {\"type\":\"string\"},\n        \"depth_path\": {\"type\":[\"string\",\"null\"]},\n        \"dyn_mask_path\": {\"type\":[\"string\",\"null\"]},\n        \"K\": {\"type\":\"array\",\"minItems\":3,\"maxItems\":3},\n        \"T_cw\": {\"type\":\"array\",\"minItems\":4,\"maxItems\":4},\n        \"scene_id\": {\"type\":\"string\"},\n        \"split\": {\"type\":\"string\"}\n    }\n}\n\nPRIMITIVE_SCHEMA = {\n    \"type\": \"object\",\n    \"required\": [\"type\",\"params\"],\n    \"properties\": {\n        \"id\": {\"type\":[\"string\",\"null\"]},\n        \"type\": {\"type\":\"string\",\"enum\":[\"plane\",\"cuboid\",\"cylinder\",\"superquadric\"]},\n        \"params\": {\"type\":\"object\"}\n    }\n}\n\ndef validate_frame_meta(meta: Dict) -> Tuple[bool, List[str]]:\n    errs = []\n    for k in FRAME_META_SCHEMA[\"required\"]:\n        if k not in meta: errs.append(f\"Missing required key: {k}\")\n    if \"K\" in meta:\n        ok = isinstance(meta[\"K\"], list) and len(meta[\"K\"])==3 and all(isinstance(r,list) and len(r)==3 for r in meta[\"K\"])\n        if not ok: errs.append(\"K must be 3x3 list\")\n    if \"T_cw\" in meta:\n        ok = isinstance(meta[\"T_cw\"], list) and len(meta[\"T_cw\"])==4 and all(isinstance(r,list) and len(r)==4 for r in meta[\"T_cw\"])\n        if not ok: errs.append(\"T_cw must be 4x4 list\")\n    return (len(errs)==0, errs)\n\ndef validate_primitive(p: Dict) -> Tuple[bool, List[str]]:\n    errs = []\n    if \"type\" not in p or \"params\" not in p:\n        return False, [\"Primitive requires 'type' and 'params'\"]\n    t = p[\"type\"]\n    params = p[\"params\"]\n    if t==\"plane\":\n        for k in [\"n\",\"d\",\"sx\",\"sy\"]:\n            if k not in params: errs.append(f\"plane missing {k}\")\n        if \"n\" in params and len(params[\"n\"])!=3: errs.append(\"plane n must be len-3\")\n    elif t==\"cuboid\":\n        for k in [\"c\",\"s\",\"q\"]:\n            if k not in params: errs.append(f\"cuboid missing {k}\")\n    elif t==\"cylinder\":\n        for k in [\"p\",\"v\",\"r\",\"h\"]:\n            if k not in params: errs.append(f\"cylinder missing {k}\")\n    elif t==\"superquadric\":\n        for k in [\"c\",\"q\",\"a\",\"e\"]:\n            if k not in params: errs.append(f\"superquadric missing {k}\")\n    return (len(errs)==0, errs)\n\nprint(\"Schemas loaded. Use validate_frame_meta() / validate_primitive().\")\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {
          "elapsed": 0
        }
      },
      "source": "import numpy as np\n\ndef normalize(v, eps=1e-8):\n    v = np.asarray(v, dtype=np.float32)\n    n = np.linalg.norm(v) + eps\n    return (v / n).astype(np.float32)\n\n# ---- Primitive parameterization helpers ----\n\ndef plane_from_params(n, d):\n    n = normalize(n)\n    d = float(d)\n    return n, d\n\ndef cuboid_from_params(c, s, q):\n    c = np.asarray(c, np.float32)\n    s = np.maximum(np.asarray(s,np.float32), 1e-3)  # sizes positive\n    q = normalize(q)                                 # unit quaternion\n    return c, s, q\n\ndef cylinder_from_params(p, v, r, h):\n    p = np.asarray(p, np.float32)\n    v = normalize(v)\n    r = float(max(r, 1e-5))\n    h = float(max(h, 1e-5))\n    return p, v, r, h\n\ndef superquadric_from_params(c, q, a, e):\n    c = np.asarray(c, np.float32)\n    q = normalize(q)\n    a = np.maximum(np.asarray(a, np.float32), 1e-4)     # axes\n    e = np.maximum(np.asarray(e, np.float32), 0.1)      # exponents lower-bounded\n    return c, q, a, e\n\n# ---- Surface sampling (very light stubs) ----\n\ndef sample_points_plane(n, d, sx, sy, num=512):\n    # Orthonormal basis on plane\n    n = normalize(n)\n    # pick arbitrary vector not parallel to n\n    a = np.array([1,0,0], np.float32) if abs(n[0])<0.9 else np.array([0,1,0], np.float32)\n    u = normalize(np.cross(n, a))\n    v = normalize(np.cross(n, u))\n    # plane point (closest to origin)\n    p0 = -d * n\n    # sample in rectangle centered at p0\n    us = (np.random.rand(num)-0.5)*sx\n    vs = (np.random.rand(num)-0.5)*sy\n    pts = p0 + np.outer(us,u) + np.outer(vs,v)\n    return pts.astype(np.float32)\n\ndef sample_points_cuboid(c, s, q, num=1024):\n    # Very rough: sample on 6 faces axis-aligned; ignoring q for simplicity in stub\n    sx, sy, sz = s\n    cx, cy, cz = c\n    n = int(num//6)\n    faces = []\n    # X faces\n    x = np.full((n,1), cx+sx/2); y = np.random.uniform(cy-sy/2, cy+sy/2,(n,1)); z = np.random.uniform(cz-sz/2, cz+sz/2,(n,1))\n    faces.append(np.hstack([x,y,z]))\n    x = np.full((n,1), cx-sx/2); y = np.random.uniform(cy-sy/2, cy+sy/2,(n,1)); z = np.random.uniform(cz-sz/2, cz+sz/2,(n,1))\n    faces.append(np.hstack([x,y,z]))\n    # Y faces\n    y = np.full((n,1), cy+sy/2); x = np.random.uniform(cx-sx/2, cx+sx/2,(n,1)); z = np.random.uniform(cz-sz/2, cz+sz/2,(n,1))\n    faces.append(np.hstack([x,y,z]))\n    y = np.full((n,1), cy-sy/2); x = np.random.uniform(cx-sx/2, cx+sx/2,(n,1)); z = np.random.uniform(cz-sz/2, cz+sz/2,(n,1))\n    faces.append(np.hstack([x,y,z]))\n    # Z faces\n    z = np.full((n,1), cz+sz/2); x = np.random.uniform(cx-sx/2, cx+sx/2,(n,1)); y = np.random.uniform(cy-sy/2, cy+sy/2,(n,1))\n    faces.append(np.hstack([x,y,z]))\n    z = np.full((n,1), cz-sz/2); x = np.random.uniform(cx-sx/2, cx+sx/2,(n,1)); y = np.random.uniform(cy-sy/2, cy+sy/2,(n,1))\n    faces.append(np.hstack([x,y,z]))\n    P = np.vstack(faces).astype(np.float32)\n    return P\n\ndef chamfer_distance(A, B):\n    # A, B: (Na,3), (Nb,3)\n    if len(A)==0 or len(B)==0:\n        return float('inf')\n    from scipy.spatial import cKDTree as KDTree  # if SciPy unavailable this will error\n    ta, tb = KDTree(A), KDTree(B)\n    da, _ = ta.query(B, k=1)\n    db, _ = tb.query(A, k=1)\n    return float(np.mean(da**2) + np.mean(db**2))\n\nprint(\"Primitive helpers ready (simple stubs).\")\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {
          "elapsed": 0
        }
      },
      "source": "import numpy as np\n\ndef greedy_match(cost):\n    # Greedy matching as a fallback when SciPy not available.\n    # cost: (M, N) matrix (pred vs gt). Returns list of (i_pred, j_gt).\n    M, N = cost.shape\n    pairs, used_r, used_c = [], set(), set()\n    for _ in range(min(M, N)):\n        i, j = np.unravel_index(np.argmin(cost + 1e6*(np.isin(np.arange(M)[:,None], list(used_r)) | np.isin(np.arange(N)[None,:], list(used_c)))), (M,N))\n        if i in used_r or j in used_c:\n            break\n        pairs.append((i,j))\n        used_r.add(i); used_c.add(j)\n    return pairs\n\ndef hungarian_match(cost):\n    try:\n        from scipy.optimize import linear_sum_assignment\n        r, c = linear_sum_assignment(cost)\n        return list(zip(r.tolist(), c.tolist()))\n    except Exception as e:\n        return greedy_match(cost)\n\n# Example usage:\n# cost = np.random.rand(5,4)\n# pairs = hungarian_match(cost)\n# print(pairs)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Model Specification: Inputs & Outputs\n\n### 5.1 Inputs\n- **RGB**: `float32` in `[0,1]`, shape **(B, 3, H, W)**.\n- **Intrinsics `K`**: shape **(B, 3, 3)**.\n- **(Optional) Dynamic mask**: **(B, 1, H, W)**.\n- **(Optional) Previous keyframe memory**: features or pose prior.\n\n### 5.2 Shared Backbone\n- ConvNeXt-T or ViT-B/16 + FPN features at strides {4, 8, 16}.\n\n### 5.3 Head A \u2014 Scene-Coordinate Regression (Pose Head)\n**Output (at stride s, e.g., s=8):**\n- **Dense scene-coords (optional)**: `(B, H/s, W/s, 3)` in **world**.\n- **Uncertainty map**: `(B, H/s, W/s, 1)`.\n**OR** (recommended for efficiency): predict on a **sparse lattice** of K pixels per frame:\n- `uv_pred`: `(B, K, 2)` pixel centers,\n- `XYZ_pred`: `(B, K, 3)` world coords,\n- `sigma_pred`: `(B, K, 1)` uncertainties.\n\n**Pose estimation:** PnP + (differentiable) RANSAC (DSAC++).  \n**Pose output:** rotation `R_cw`, translation `t_cw` (or SE(3)/Sim(3) if monocular scale handling).\n\n### 5.4 Head B \u2014 Primitive Set-Prediction (Mapping Head)\n- Up to **M** slots, DETR-style bipartite matching with a **no-object** class.\n- For each slot: `type` \u2208 {none, plane, cuboid, cylinder, superquadric} and a parameter vector.\n- We predict primitives in the **camera frame** and transform to **world** with the pose from Head A.\n\n**Slot outputs (per item):**\n- `type_logits`: `(B, M, T)` where `T=1+num_types` (no-object + types)\n- `params`: flattened vector per type; packed as `(B, M, P_max)` with masks\n- `conf`: `(B, M, 1)` confidence\n\n**Typical parameterizations (camera frame):**\n- Plane: unit normal `n`, offset `d`, extents `(sx,sy)`; reparam via `n = v/||v||`, `sx,sy = exp(.)`.\n- Cuboid: center `c`, size `s=exp(.)`, rotation quaternion `q/||q||`.\n- Cylinder: axis dir `v/||v||`, point `p`, `r=softplus(.)`, `h=softplus(.)`.\n- Superquadric: `c`, `q`, semi-axes `a_i=softplus(.)`, exponents `eps_i=softplus(.)` with bounds.\n\nAfter pose composition: **world-frame primitives** are sent to data association & map update.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Losses\n\n### 6.1 Scene-Coordinate / Pose Losses\n- **Scene-coord regression:** L1 on valid pixels:  \\(\\mathcal{L}_{xyz} = \\frac{1}{N}\\sum \\|\\hat{\\mathbf{X}}_w - \\mathbf{X}_w\\|_1\\).\n- **Uncertainty weighting:** scale residuals by predicted \\(\\sigma\\): \\(\\mathcal{L}_{unc} = \\sum \\frac{\\|\\Delta\\|_2^2}{\\sigma^2} + \\log \\sigma^2\\).\n- **DSAC pose loss:** negative log inlier probability and reprojection residual under best hypothesis.\n- **Reprojection auxiliary:** \\(\\mathcal{L}_{reproj} = \\sum \\|\\pi(K, T_{cw}, \\hat{\\mathbf{X}}_w) - \\mathbf{u}\\|_1\\).\n\n### 6.2 Primitive Set Losses (Hungarian-matched)\nFor a predicted set \\( \\hat{\\mathcal{S}} \\) and GT set \\( \\mathcal{S} \\):\n- **Type/class CE**: classification cost.\n- **Param losses** (per type): L1/L2 on canonical params (e.g., plane normal cosine, offsets, sizes).\n- **Shape similarity**: Chamfer distance between sampled surface points.\n- **Silhouette/occupancy**: rasterize to a coarse grid (in camera or world) and use BCE/IoU.\n- **Regularizers**: unit-norm for normals/axis/quaternion, positivity for sizes/radii/axes (via softplus).\n\n**Total:** \\(\\mathcal{L} = \\lambda_{xyz}\\mathcal{L}_{xyz} + \\lambda_{pose}\\mathcal{L}_{pose} + \\lambda_{cls}\\mathcal{L}_{cls} + \\lambda_{param}\\mathcal{L}_{param} + \\lambda_{geom}\\mathcal{L}_{chamfer} + \\lambda_{sil}\\mathcal{L}_{sil}\\).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Training Schedule & Augmentation\n- **Backbone**: ImageNet init; AdamW (lr 2e-4, wd 1e-2), cosine decay; 150\u2013200k iters.\n- **Batch**: 8\u201316 @ 512\u00d7512 (mixed precision).\n- **Sampling**: K=1\u20132k scene-coord pixels per image (avoid dynamic mask).\n- **Augmentations**: color jitter, blur, noise, exposure, small intrinsics jitter; motion blur; random occluders.\n- **Balancing**: gradient normalization or loss weights (\\(\\lambda\\)) tuned to equalize magnitudes across heads.\n- **Curriculum**: start with planes & cuboids; add cylinders/superquadrics in stage 2.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Inference & Mapping\n**Per frame:**\n1. Dynamic mask (if available).\n2. Head A \u2192 scene-coords \u2192 PnP/DSAC \u2192 pose \\(T_{cw}\\). If inliers low, keep last good pose and increase reliance on step 4.\n3. Head B \u2192 M primitives in **camera**; transform to **world** with \\(T_{cw}\\).\n4. **Data association** with existing primitives (IoU in BEV/silhouette, Chamfer on samples). Update via EKF/LM. Spawn new primitives as needed.\n5. Periodically: image retrieval \u2192 loop closure \u2192 pose-graph optimization (Sim(3) for mono). Jointly adjust primitive params.\n\n**Outputs to downstream:**\n- Current pose; **compact map**: list of primitives with covariances.\n- Optional coarse ESDF/occupancy derived from primitives for planners.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Evaluation\n- **Pose**: ATE (abs trajectory error), RPE (rel pose error), rotation/translation medians.\n- **Primitives**: Chamfer on sampled surfaces; silhouette IoU; plane normal error; cuboid IoU; cylinder axis angle; **map bitrate** (floats \u00d7 4B).\n- **Navigation**: collision-free rate on planned paths; clearance min/max; ESDF coverage.\n- **Ablations**: no dynamics mask; no superquadrics; DSAC vs RANSAC; sparse vs dense scene-coords; set size M.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10. Repro Checklist\n- Export RGB/K/T_cw, dynamic masks, scene-coords (sparse), and primitive GT (per-frame or per-scene).\n- Verify JSON schemas with the validators below.\n- Start training with {planes, cuboids} and M=32 slots; H=W=512; K=2k scene-coord points.\n- Evaluate on held-out sequences; report pose + primitive metrics + map size.\n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo": {
          "elapsed": 0
        }
      },
      "source": "# Example: build a tiny example frame meta and primitive list and validate\nmeta = {\n  \"frame_id\": 1,\n  \"timestamp\": 1725001234.0,\n  \"rgb_path\": \"images/000001.png\",\n  \"depth_path\": None,\n  \"dyn_mask_path\": None,\n  \"K\": [[600.0,0.0,320.0],[0.0,600.0,240.0],[0.0,0.0,1.0]],\n  \"T_cw\": [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],\n  \"scene_id\": \"scene_000\",\n  \"split\": \"train\"\n}\nok, errs = validate_frame_meta(meta)\nprint(\"Frame meta valid:\", ok, errs)\n\nprims = [\n  {\"id\":\"pl0\",\"type\":\"plane\",\"params\":{\"n\":[0,0,1],\"d\":0.0,\"sx\":20.0,\"sy\":20.0}},\n  {\"id\":\"cb1\",\"type\":\"cuboid\",\"params\":{\"c\":[5,2,0],\"s\":[4,3,10],\"q\":[1,0,0,0]}}\n]\nfor p in prims:\n    ok, errs = validate_primitive(p)\n    print(p[\"id\"], \"valid:\", ok, errs)\n",
      "outputs": [],
      "execution_count": null
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "name": "PRISM_SLAM_Colab_UPDATED.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# PRISM-SLAM \u2014 Hybrid Scene-Coordinate + Primitive-Shape Mapping\n**A Colab-style short paper with dataset & model I/O specs (Updated)**  \n*Draft \u2014 August 2025*\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Abstract\nWe propose a hybrid visual localization and mapping approach that combines (i) a **scene-coordinate head** for geometry-tight 6-DoF pose estimation via PnP/RANSAC, and (ii) a **primitive-shape head** that predicts a small set of parametric 3D shapes (planes, cuboids, cylinders, superquadrics). The result is a **compact global map** that is well-suited for navigation and planning, without photorealistic detail or large point clouds.\nWe provide a concrete dataset format, precise **model inputs/outputs**, loss definitions, training schedule, and evaluation metrics. The notebook includes reference code stubs for dataset validation, primitive parameterization, silhouette rasterization ideas, and set-matching (Hungarian fallback).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. Motivation \u2014 Realtime World Model (Monocular)\nWe target a **realtime world model** built from **monocular RGB**, producing both the **camera pose** and a **compact map of static geometry** while being **robust to dynamic objects**.\n\nWe propose a **progressive, anytime pipeline** with **multi-level outputs** emitted at different points during a single forward pass:\n\n- **Level 0 \u2014 Coarse anti-collision (early exit, highest FPS)**: obstacle likelihood / free-space in a coarse frustum or BEV, plus a dynamic/static mask.  \n- **Level 1 \u2014 Medium resolution + classification + map registration**: primitives (planes/cuboids/cylinders/superquadrics) with **class labels**, and a **pose update** (scene-coordinates \u2192 PnP).  \n- **Level 2 \u2014 High resolution identification (lowest FPS)**: fine-grained categories or attributes within class, shape refinements, and optional instance tracking.\n\nLevels have **different update rates** (e.g., L0 ~ 30\u201360 Hz, L1 ~ 10\u201320 Hz, L2 ~ 2\u201310 Hz). The model **emits them progressively** via **early-exit heads**; downstream logic consumes the freshest available level.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0.1 Progressive Multi-Level Outputs \u2014 Is it possible?\n**Yes.** Implement with **early-exit / anytime prediction** over a shared backbone:\n\n- **Backbone + FPN** (strides 4/8/16).  \n- **Head L0 (early):** at stride 16/32 \u2192 small decoder \u2192 coarse occupancy/free-space + dynamic mask.  \n- **Head L1 (mid):** at stride 8 \u2192 set-prediction primitives (+ classes) and **scene-coordinates** + uncertainty for **PnP/DSAC**.  \n- **Head L2 (late):** at stride 4 \u2192 refinements (primitive params, sub-class IDs, attributes).\n\n**Training**: anytime distillation (L0/L1 mimic L2), loss balancing (uncertainty/GradNorm), dynamic masking, latency budgeting.  \n**Runtime**: emit **L0 ASAP** for safety, **L1** to update pose & map, **L2** opportunistically.\n\n### Pseudocode sketch\n```python\nfeatures = backbone(rgb)\nl0 = head_l0(features[\"p16\"]); emit(\"L0\", l0)\nl1 = head_l1(features[\"p8\"]); pose = solve_pnp(l1.scene_coords, K); emit(\"pose\", pose); emit(\"L1\", l1)\nif budget_allows():\n    l2 = head_l2(features[\"p4\"]); emit(\"L2\", l2)\n```\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Introduction\nTraditional SLAM often stores large **point clouds** or **voxel volumes**. For embedded navigation, we target a **compact map** made of **few parametric shapes** while keeping tight pose accuracy via **scene-coordinates** (PnP/DSAC). Dynamics are masked out.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Related Work (brief)\nScene Coordinates (DSAC/DSAC++), primitive mapping (planes, cuboids, cylinders, superquadrics), set prediction (DETR), and loop-closure with pose-graphs.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Approach Overview\nShared backbone \u2192 two heads:\n1) **Scene-Coord Head:** sparse 2D\u21923D with uncertainty \u2192 PnP(+RANSAC/DSAC) for pose.  \n2) **Primitive Set Head:** predicts planes/cuboids/cylinders/superquadrics (camera frame), transformed to world with pose; data-associated into a **shape-graph** map. Loop closures via retrieval \u2192 PGO (Sim(3) for mono).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Dataset Specification\n- **Camera (OpenCV):** +x right, +y down, +z forward.  **World (ENU):** +X east, +Y north, +Z up.\n- **Per-frame meta** (`meta/{frame}.json`): `frame_id`, `rgb_path`, optional `depth_path`, `dyn_mask_path`, `K (3x3)`, `T_cw (4x4)`, `scene_id`, `split`.\n- **Scene-coordinates** (`scene_coords/{frame}.npz`): `uv (N,2)`, `XYZ (N,3)`, `valid (N,)`, optional `cov (N,3,3)`.\n- **Primitives** (`primitives/{frame}.json` or per-scene): items with `type` \u2208 {plane,cuboid,cylinder,superquadric} and their parameters.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Lightweight JSON schema validators (example)\nimport json\n\nFRAME_META_SCHEMA = {\n    \"type\": \"object\",\n    \"required\": [\"frame_id\",\"rgb_path\",\"K\",\"T_cw\",\"scene_id\"],\n}\nPRIMITIVE_SCHEMA = {\n    \"type\": \"object\",\n    \"required\": [\"type\",\"params\"],\n}\ndef validate_frame_meta(meta):\n    errs = []\n    for k in FRAME_META_SCHEMA[\"required\"]:\n        if k not in meta: errs.append(f\"Missing {k}\")\n    return len(errs)==0, errs\n\ndef validate_primitive(p):\n    if \"type\" not in p or \"params\" not in p:\n        return False, [\"Primitive requires 'type' and 'params'\"]\n    return True, []\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Primitive helpers (stubs)\nimport numpy as np\n\ndef _norm(v, eps=1e-8):\n    v = np.asarray(v, np.float32); n = np.linalg.norm(v) + eps; return (v/n).astype(np.float32)\n\ndef plane_from_params(n, d): return _norm(n), float(d)\ndef cuboid_from_params(c, s, q):\n    return np.asarray(c,np.float32), np.maximum(np.asarray(s,np.float32),1e-3), _norm(q)\ndef cylinder_from_params(p, v, r, h):\n    return np.asarray(p,np.float32), _norm(v), float(max(r,1e-5)), float(max(h,1e-5))\ndef superquadric_from_params(c, q, a, e):\n    return np.asarray(c,np.float32), _norm(q), np.maximum(np.asarray(a,np.float32),1e-4), np.maximum(np.asarray(e,np.float32),0.1)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Model: Inputs & Outputs\n**Inputs**: RGB (B,3,H,W); K (B,3,3); optional dynamic mask.  \n**Head A (scene-coords)**: predict K sparse 3D points + uncertainty \u2192 **PnP/DSAC** \u2192 pose T_cw.  \n**Head B (primitives)**: M slots, set-prediction (types + params + conf), predicted in camera frame \u2192 transformed to world via T_cw.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Losses\n- Scene-coords: L1 on XYZ + reprojection; uncertainty weighting; DSAC pose loss.  \n- Primitives: classification CE; parameter L1/L2; Chamfer on sampled surfaces; silhouette/occupancy BCE/IoU; regularizers (unit-norm, positivity).  \n- Total: weighted sum with GradNorm/uncertainty balancing.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Training\nAdamW (lr 2e-4, wd 1e-2), cosine; 150\u2013200k iters; batch 8\u201316 @ 512\u00b2.  \nAugment: color jitter, noise/blur, intrinsics jitter; dynamic masks.  \nCurriculum: planes/cuboids first, then cylinders/superquadrics.  \nAnytime distillation for L0/L1 from L2 if present.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Inference & Mapping\nPer frame: L0 early for anti-collision; L1 \u2192 pose+primitives update; L2 refinements if budget.  \nData-associate primitives to a **shape-graph**; loop closures via retrieval \u2192 PGO; optional coarse ESDF for planners.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Evaluation\nPose: ATE/RPE.  Primitives: Chamfer/IoU/normal error; map size (bytes).  \nNavigation: collision-free rate; ESDF coverage.  Ablations: dynamics off, no superquadrics, etc.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10. Repro Checklist\n- Export RGB/K/T_cw, dynamic masks, scene-coords, primitive GT.  \n- Validate JSONs; train with M=32, K=2k, H=W=512.  \n- Report pose + primitive metrics + map size.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Example: validate a tiny meta + primitive JSON in-memory\nmeta = {\"frame_id\":1,\"rgb_path\":\"images/000001.png\",\"K\":[[600,0,320],[0,600,240],[0,0,1]],\"T_cw\":[[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],\"scene_id\":\"scene_000\"}\nok, errs = validate_frame_meta(meta); print(\"meta valid:\", ok, errs)\np = {\"type\":\"plane\",\"params\":{\"n\":[0,0,1],\"d\":0.0,\"sx\":20.0,\"sy\":20.0}}\nok, errs = validate_primitive(p); print(\"primitive valid:\", ok, errs)\n",
      "outputs": [],
      "execution_count": null
    }
  ]
}